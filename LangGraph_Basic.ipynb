{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMn08dCCaFfVlUuapb7G7/6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Afzalfaizi/LangGraph/blob/main/LangGraph_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q-C3vxvWuk3A"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -q langchain_google_genai langchain_core langchain_community tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "Hn4iS_Z8u1hL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result  = llm.invoke(\"Who won 2024 US presidential ellection?\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyOkCqSeu9jN",
        "outputId": "ea6f5917-1e07-48ca-8445-d3d4a72ca2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The 2024 US presidential election has not yet happened.  The election will be held in November 2024.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-9e8f9288-5ae7-4ad5-8eb1-04a4aeecd32d-0', usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat models in LangChain have a number of default methods. For the most part, we'll be using:\n",
        "\n",
        "stream: stream back chunks of the response\n",
        "invoke: call the chain on an input\n",
        "And, as mentioned, chat models take messages as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
      ],
      "metadata": {
        "id": "zAvvNdWU6VwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        " # create a message\n",
        "messages = [\n",
        "    HumanMessage(content=\"Hi\",  name = \"Faizi \"),\n",
        "    AIMessage(content='Hi there! How can I help you today?\\n', name= \"AI Assistan\"),\n",
        "    HumanMessage(content=\"who is Imran Khan?\", name = \"Faizi\"),\n",
        "    AIMessage(content=\"Imran Khan is a Pakistani politician who served as the 22nd prime minister of Pakistan from 2018 to 2022.  Before entering politics, he was a highly successful international cricketer, leading the Pakistan cricket team to victory in the 1992 Cricket World Cup.\", name= \"AI Assistan\"),\n",
        "    HumanMessage(content=\"Do you Know about his 1st wife?\", name = \"Faizi\"),\n",
        "    AIMessage(content=\"Imran Khan's first wife was Jemima Goldsmith.  She is a British socialite, film producer, and writer.  Their marriage lasted from 1995 to 2004.\", name= \"AI Assistan\"),\n",
        "    HumanMessage(content=\"Do you know about his 2nd wife?\", name = \"Faizi\"),\n",
        "\n",
        "    ]\n",
        "\n",
        "\n",
        "llm.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHJg3DXn6R7z",
        "outputId": "dc1e7839-57d5-4b2c-c418-02a2700de573"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Imran Khan's second wife was Reham Khan.  She is a Pakistani journalist and writer. Their marriage was relatively short-lived, lasting from 2015 to 2015.\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-c438e4ab-283a-4e47-b962-5a0d976dcba3-0', usage_metadata={'input_tokens': 150, 'output_tokens': 44, 'total_tokens': 194, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search Tools**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mdgujMeIFQ1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll also see Tavily in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. Some lessons (in Module 4) will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself.\n",
        "\n"
      ],
      "metadata": {
        "id": "zgFR0wTvFdIz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "issfC7WYFXow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yEARbGhRFT80"
      }
    }
  ]
}