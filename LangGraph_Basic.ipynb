{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+QLLB7q+5oUs5naKIAhvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Afzalfaizi/LangGraph/blob/main/LangGraph_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-C3vxvWuk3A"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -q langchain_google_genai langchain_core langchain_community tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "Hn4iS_Z8u1hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result  = llm.invoke(\"Who won pakistan last election?\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyOkCqSeu9jN",
        "outputId": "f8f83894-2d17-43c8-9e71-4a8b29c15531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The 2024 US presidential election has not yet happened.  The election will be held in November 2024.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-0aeb0668-85ad-439d-bedb-e356a443610e-0', usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat models in LangChain have a number of default methods. For the most part, we'll be using:\n",
        "\n",
        "stream: stream back chunks of the response\n",
        "invoke: call the chain on an input\n",
        "And, as mentioned, chat models take messages as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
      ],
      "metadata": {
        "id": "zAvvNdWU6VwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        " # create a message\n",
        "messages = [\n",
        "    HumanMessage(content=\"Hi\",  name = \"Faizi \"),\n",
        "    AIMessage(content='Hi there! How can I help you today?\\n', name= \"AI Assistan\"),\n",
        "    HumanMessage(content=\"who is Imran Khan?\", name = \"Faizi\"),\n",
        "    AIMessage(content=\"Imran Khan is a Pakistani politician who served as the 22nd prime minister of Pakistan from 2018 to 2022.  Before entering politics, he was a highly successful international cricketer, leading the Pakistan cricket team to victory in the 1992 Cricket World Cup.\", name= \"AI Assistan\"),\n",
        "    HumanMessage(content=\"Do you Know about his 1st wife?\", name = \"Faizi\"),\n",
        "    AIMessage(content=\"Imran Khan's first wife was Jemima Goldsmith.  She is a British socialite, film producer, and writer.  Their marriage lasted from 1995 to 2004.\", name= \"AI Assistan\"),\n",
        "    HumanMessage(content=\"Do you know about his 2nd wife?\", name = \"Faizi\"),\n",
        "\n",
        "    ]\n",
        "\n",
        "\n",
        "llm.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHJg3DXn6R7z",
        "outputId": "5143402c-a8ea-4451-97f3-704684c1cab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Imran Khan's second wife was Reham Khan.  She is a Pakistani journalist and writer. Their marriage was relatively short-lived, lasting from 2015 to 2015.\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-c22e4d5b-c112-47c5-9526-035dd085f138-0', usage_metadata={'input_tokens': 150, 'output_tokens': 44, 'total_tokens': 194, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search Tools**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mdgujMeIFQ1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll also see Tavily in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. Some lessons (in Module 4) will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself.\n",
        "\n"
      ],
      "metadata": {
        "id": "zgFR0wTvFdIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n",
        "tavily_search = TavilySearchResults(max_results=3)\n",
        "search_docs = tavily_search.invoke(\"what is langGraph?\")\n"
      ],
      "metadata": {
        "id": "issfC7WYFXow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WOBiyaWX15s",
        "outputId": "7fee1c1b-68fb-4927-98aa-8a6ee45a8aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
              "  'content': 'LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.'},\n",
              " {'url': 'https://towardsdatascience.com/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787',\n",
              "  'content': \"LangGraph is a low-level framework that offers extensive customisation options, allowing you to build precisely what you need. Since LangGraph is built on top of LangChain, it's seamlessly integrated into its ecosystem, making it easy to leverage existing tools and components. However, there are areas where LangGrpah could be improved:\"},\n",
              " {'url': 'https://langchain-ai.github.io/langgraph/',\n",
              "  'content': 'OverviewÂ¶. LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yEARbGhRFT80"
      }
    }
  ]
}